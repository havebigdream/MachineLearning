# 机器学习

## 1，Scikit-learn

### 1.1 用户指南

#### 1.1.1数据集加载实用程序

##### ① 生成的数据集

1）单标签数据集

```

#--------------------------------------------单标签----------------------------------------

	make_blobs()和make_classification()通过为每个类别分配一个或多个点的正态分布簇来创建多类别数据集。 make_blobs()对每个聚类的中心和标准偏差有更好的控制，并用于阐述、讲解聚类。 make_classification()专于通过相关，冗余和非信息性特征引入噪音；每个类别有多个高斯聚类、以及特征空间的线性变换。

	make_gaussian_quantiles()将单个高斯聚类划分为由同心超球体分隔的近似相等大小的类。 make_hastie_10_2()生成类似的二进制的，10维问题。

	make_circles()和make_moons()生成对某些算法（例如基于质心的聚类或线性分类)具有挑战性的二维二进制分类数据集,包括可选的高斯噪声。它们对于可视化很有用。make_circles()产生具有球形决策边界的高斯数据以进行二元分类，同时 make_moons()产生两个交织的半圆。
```

```python
import matplotlib.pyplot as plt

from sklearn.datasets import make_classification
from sklearn.datasets import make_blobs
from sklearn.datasets import make_gaussian_quantiles

plt.figure(figsize=(8, 8))
plt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)

plt.subplot(321)
plt.title("One informative feature, one cluster per class", fontsize='small')
X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=1,
                             n_clusters_per_class=1)
plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,
            s=25, edgecolor='k')

plt.subplot(322)
plt.title("Two informative features, one cluster per class", fontsize='small')
X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,
                             n_clusters_per_class=1)
plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,
            s=25, edgecolor='k')

plt.subplot(323)
plt.title("Two informative features, two clusters per class",
          fontsize='small')
X2, Y2 = make_classification(n_features=2, n_redundant=0, n_informative=2)
plt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2,
            s=25, edgecolor='k')

plt.subplot(324)
plt.title("Multi-class, two informative features, one cluster",
          fontsize='small')
X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,
                             n_clusters_per_class=1, n_classes=3)
plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,
            s=25, edgecolor='k')

plt.subplot(325)
plt.title("Three blobs", fontsize='small')
X1, Y1 = make_blobs(n_features=2, centers=3)
plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,
            s=25, edgecolor='k')

plt.subplot(326)
plt.title("Gaussian divided into three quantiles", fontsize='small')
X1, Y1 = make_gaussian_quantiles(n_features=2, n_classes=3)
plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,
            s=25, edgecolor='k')

plt.show()

'''
The first 4 plots use the make_classification with different numbers of informative features, clusters per class and classes. The final 2 plots use make_blobs and make_gaussian_quantiles.
	这个例子画了集中随机生成的分类数据集，为了方便可视化，所有的数据集都有两个特征（两个特征是为了在在二维直角坐标系上可视化），不同的颜色代表不同的类别标签
	前四个图使用make_classification()生成只是带有不同数量的信息功能，每个类和每个类的聚类。 最后两个图使用make_blobs()和make_gaussian_quantiles()。
	
'''
```

![单标签数据集](picture\单标签数据集.jpg)

2）多标签数据集

```
# ----------------------------------多标签数据集---------------------------------
1，基本用法
sklearn.datasets.make_multilabel_classification（n_samples = 100，n_features = 20，*，n_classes = 5，n_labels = 2，length = 50，allow_unlabeled = True，sparse = False，return_indicator ='dense'，return_distributions = False，random_state = None ）
2，生成随机的多标签分类问题。
     对于每个样本，生成过程为：
        ① 选择标签数：n〜Poisson（n_labels）
        ② n次，选择一个类c：c〜多项式（theta）
        ③ 选择文档长度：k〜泊松（长度）
        ④ k次，选择一个单词：w〜多项式（theta_c）
在上述过程中，使用拒绝抽样来确保n永远不为零或大于n_classes，并且文档长度永远不为零。同样，我们拒绝已经选择的类。

3，'''
---------------------------------------------Params----------------------------------
参数：
            n_samples int，可选（默认值= 100）
            样本数。

            n_features int，可选（默认值= 20）
            特征总数。

            n_classes int，可选（默认值= 5）
            分类问题的类数。

            n_labels int，可选（默认值= 2）
            每个实例的平均标签数。更准确地说，每个样本的标签数量n_labels均以Poisson分布 作为其期望值，但样本的边界（使用拒绝抽样）为n_classes，如果allow_unlabeled为False ，则必须为非零 。

            长度int，可选（默认为50）
            特征的总和（如果是文档，则为单词数）是从具有此期望值的泊松分布中得出的。

            allow_unlabeled 布尔值，可选（默认为True）
            如果为True，则某些实例可能不属于任何类。

            稀疏布尔值，可选（默认= False）
            如果为True，则返回一个稀疏特征矩阵

            return_indicator '密集'（默认）| '稀疏'| 假
            如果以密集的二进制指示符格式dense返回Y。如果 以稀疏二进制指示符格式'sparse'返回Y。 False返回标签列表的列表。

            return_distributions bool，可选（默认为False）
            如果为True，则返回给定类别的要素的先验类别概率和条件概率，并从中得出数据。

            random_state int，RandomState实例，默认=无
            确定用于生成数据集的随机数生成。为多个函数调用传递可重复输出的int值
            
 --------------------------------------return------------------------------------------------------------
 返回值：		
                X 形状的数组[n_samples，n_features]
                生成的样本。

                Y 数组或形状为[n_samples，n_classes]的稀疏CSR矩阵
                标签集。

                p_c 数组，形状[n_classes]
                绘制每个类的概率。仅在时返回 return_distributions=True。

                p_w_c 数组，形状[n_features，n_classes]
                给每个类别绘制每个要素的概率。仅在时返回return_distributions=True。
'''


#---------------------------------生成多标签数据集代码--------------------------
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_multilabel_classification as make_ml_clf


COLORS = np.array(['!',
                   '#FF3333',  # red
                   '#0198E1',  # blue
                   '#BF5FFF',  # purple
                   '#FCD116',  # yellow
                   '#FF7216',  # orange
                   '#4DBD33',  # green
                   '#87421F'   # brown
                   ])

# Use same random seed for multiple calls to make_multilabel_classification to
# ensure same distributions
RANDOM_SEED = np.random.randint(2 ** 10)


def plot_2d(ax, n_labels=1, n_classes=3, length=50):
    X, Y, p_c, p_w_c = make_ml_clf(n_samples=150, n_features=2,
                                   n_classes=n_classes, n_labels=n_labels,
                                   length=length, allow_unlabeled=False,
                                   return_distributions=True,
                                   random_state=RANDOM_SEED)

    ax.scatter(X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]
                                                    ).sum(axis=1)),
               marker='.')
    ax.scatter(p_w_c[0] * length, p_w_c[1] * length,
               marker='*', linewidth=.5, edgecolor='black',
               s=20 + 1500 * p_c ** 2,
               color=COLORS.take([1, 2, 4]))
    ax.set_xlabel('Feature 0 count')
    return p_c, p_w_c


_, (ax1, ax2) = plt.subplots(1, 2, sharex='row', sharey='row', figsize=(8, 4))
plt.subplots_adjust(bottom=.15)

p_c, p_w_c = plot_2d(ax1, n_labels=1)
ax1.set_title('n_labels=1, length=50')
ax1.set_ylabel('Feature 1 count')

plot_2d(ax2, n_labels=3)
ax2.set_title('n_labels=3, length=50')
ax2.set_xlim(left=0, auto=True)
ax2.set_ylim(bottom=0, auto=True)

plt.show()

print('The data was generated from (random_state=%d):' % RANDOM_SEED)
print('Class', 'P(C)', 'P(w0|C)', 'P(w1|C)', sep='\t')
for k, p, p_w in zip(['red', 'blue', 'yellow'], p_c, p_w_c.T):
    print('%s\t%0.2f\t%0.2f\t%0.2f' % (k, p, p_w[0], p_w[1]))
```

![](picture\多标签数据集.jpg)

```
#------------------------------多标签文档分类问题------------------------------
数据集是根据以下过程随机生成的：

        选择标签数：n〜Poisson（n_labels）

        n次，选择一个类c：c〜多项式（theta）

        选择文档长度：k〜泊松（长度）

        k次，选择一个单词：w〜多项式（theta_c）

在上述过程中，使用拒绝采样来确保n大于2，并且文档长度永远不会为零。同样，我们拒绝已经选择的类。分配给两个类别的文档被两个彩色圆圈包围。

通过将PCA和CCA发现的前两个主要成分投影用于可视化目的，然后通过使用sklearn.multiclass.OneVsRestClassifier带有两个带有线性核的SVC 的元分类器来学习每个类的判别模型，来执行分类。请注意，PCA用于执行无监督的降维，而CCA用于执行无监督的降维。

注意：在图中，“未标记的样本”并不意味着我们不知道标记（如在半监督学习中一样），而是样本根本没有 标记。	
```

```python
print(__doc__)

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_multilabel_classification
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import CCA


def plot_hyperplane(clf, min_x, max_x, linestyle, label):
    # get the separating hyperplane
    w = clf.coef_[0]
    a = -w[0] / w[1]
    xx = np.linspace(min_x - 5, max_x + 5)  # make sure the line is long enough
    yy = a * xx - (clf.intercept_[0]) / w[1]
    plt.plot(xx, yy, linestyle, label=label)


def plot_subfigure(X, Y, subplot, title, transform):
    if transform == "pca":
        X = PCA(n_components=2).fit_transform(X)
    elif transform == "cca":
        X = CCA(n_components=2).fit(X, Y).transform(X)
    else:
        raise ValueError

    min_x = np.min(X[:, 0])
    max_x = np.max(X[:, 0])

    min_y = np.min(X[:, 1])
    max_y = np.max(X[:, 1])

    classif = OneVsRestClassifier(SVC(kernel='linear'))
    classif.fit(X, Y)

    plt.subplot(2, 2, subplot)
    plt.title(title)

    zero_class = np.where(Y[:, 0])
    one_class = np.where(Y[:, 1])
    plt.scatter(X[:, 0], X[:, 1], s=40, c='gray', edgecolors=(0, 0, 0))
    plt.scatter(X[zero_class, 0], X[zero_class, 1], s=160, edgecolors='b',
                facecolors='none', linewidths=2, label='Class 1')
    plt.scatter(X[one_class, 0], X[one_class, 1], s=80, edgecolors='orange',
                facecolors='none', linewidths=2, label='Class 2')

    plot_hyperplane(classif.estimators_[0], min_x, max_x, 'k--',
                    'Boundary\nfor class 1')
    plot_hyperplane(classif.estimators_[1], min_x, max_x, 'k-.',
                    'Boundary\nfor class 2')
    plt.xticks(())
    plt.yticks(())

    plt.xlim(min_x - .5 * max_x, max_x + .5 * max_x)
    plt.ylim(min_y - .5 * max_y, max_y + .5 * max_y)
    if subplot == 2:
        plt.xlabel('First principal component')
        plt.ylabel('Second principal component')
        plt.legend(loc="upper left")


plt.figure(figsize=(8, 6))

X, Y = make_multilabel_classification(n_classes=2, n_labels=1,
                                      allow_unlabeled=True,
                                      random_state=1)

plot_subfigure(X, Y, 1, "With unlabeled samples + CCA", "cca")
plot_subfigure(X, Y, 2, "With unlabeled samples + PCA", "pca")

X, Y = make_multilabel_classification(n_classes=2, n_labels=1,
                                      allow_unlabeled=False,
                                      random_state=1)

plot_subfigure(X, Y, 3, "Without unlabeled samples + CCA", "cca")
plot_subfigure(X, Y, 4, "Without unlabeled samples + PCA", "pca")

plt.subplots_adjust(.04, .02, .97, .94, .09, .2)
plt.show()
```

![](picture\多标签数据集分类.jpg)